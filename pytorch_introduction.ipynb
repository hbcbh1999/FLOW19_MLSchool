{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tutorial\n",
    "*Adapted from [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html \"Pytorch tutorial\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is:  \n",
    "* A deep learning framework (mainly geared towards research)  \n",
    "* A GPU-powered numpy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from Numpy ndarrays, Tensors can be used on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,4, dtype=torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.2, 4],[3, 5.6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be easily manipulated using usual operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())\n",
    "\n",
    "y = torch.rand(2,2)\n",
    "print(x + y)\n",
    "# alternatively\n",
    "z =  torch.empty(2,2)\n",
    "torch.add(x,y, out=z)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.size())\n",
    "\n",
    "# to reshape a multi-dimensional array\n",
    "z = z.view(4)     # the size -1 can be used, the dimension is inferred from the others\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Numpy to Pytorch variables share the memory location, hence we can easily **modify** them, as long as they are on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,2)\n",
    "print(a)\n",
    "\n",
    "# from Pytorch to Numpy\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(3)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Numpy to Pytorch\n",
    "c = torch.from_numpy(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``autograd`` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,4, dtype=torch.float)\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 7\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor``.\n",
    "Other important methods:  \n",
    "* ``.backward()``: gradients are computed automatically\n",
    "* ``.detach()``: future computations are not tracked\n",
    "\n",
    "We can also prevent tracking history wrapping the code in a block in ``with torch.no_grad():``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of the ``torch.nn`` package. An ``nn.Module`` contains layers, and a method ``forward(input)`` that returns the ``output``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for neural networks\n",
    "1. Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "2. Iterate over a dataset of inputs, for each input:\n",
    "  1. Process input through the network\n",
    "  2. Compute the loss (how far is the output from being correct)\n",
    "  3. Propagate gradients back into the networkâ€™s parameters\n",
    "  4. Update the weights of the network, using some stochastic gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Example of network definition and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net_MNIST(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1):\n",
    "        super(Net_MNIST, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "# create a Net object, i.e. our neural network\n",
    "input_dim = 28**2   # assuming as input an 28x28 image\n",
    "hidden_dim = 30\n",
    "batch_size = 1\n",
    "output_dim = 10\n",
    "\n",
    "net = Net_MNIST(input_dim=input_dim, hidden_dim=hidden_dim, batch_size=batch_size, output_dim=output_dim)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each mini-batch of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Processing input\n",
    "The network is called as a function, using the input batch data to get the output batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(batch_size, 28, 28)\n",
    "out = net(inp.view(-1,input_dim))\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Computing loss\n",
    "There are a number of pre-defined loss functions, but it is possible to define our own.\n",
    "It is important that, given the prediction $y_{pred} \\in \\mathbb{R}^d$ and the ground truth $y_{true} \\in \\mathbb{R}^d$, the loss $\\mathcal{L}$ is defined so that\n",
    "\n",
    "$$ \\mathcal{L} : \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(10)         # a dummy target, for example\n",
    "target = target.view(1, -1)      # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Backpropagation\n",
    "The computation of the gradient is done automatically using the ``.backward()`` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Update the weights\n",
    "We need to choose an optimization algorithm, different options are available as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in the training loop:\n",
    "optimizer.zero_grad()             # zero the gradient buffers\n",
    "out = net(inp.view(-1,input_dim))\n",
    "loss = criterion(out, target)\n",
    "loss.backward()\n",
    "optimizer.step()                  # does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
